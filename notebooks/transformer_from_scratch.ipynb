{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5f67bb",
   "metadata": {},
   "source": [
    "### Transformer Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e25622",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eaaf76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System dependencies\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Torch dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Data loaders\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a91279",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65b8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    ### Data config\n",
    "    \"num_classes\": 4,\n",
    "    \n",
    "    ### NLP config\n",
    "    \"max_vocab_size\": 20000,\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"max_seq_len\": 128,\n",
    "    \n",
    "    ### system settings\n",
    "    \"seed\": 42,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b41061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    torch.manual_seed(CONFIG[\"seed\"])\n",
    "    \n",
    "setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c516e31",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce625437",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "label_map = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c971714",
   "metadata": {},
   "source": [
    "#### Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b004f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer\n",
    "\n",
    "def tokenize(text):\n",
    "    # text.lower() - lowercase all the text samples\n",
    "    # text.split() - split the sentence into words\n",
    "    return text.lower().split() # [\"word1\", \"word2\", \"word3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a03b1271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]['text'] # sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ca04239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wall',\n",
       " 'st.',\n",
       " 'bears',\n",
       " 'claw',\n",
       " 'back',\n",
       " 'into',\n",
       " 'the',\n",
       " 'black',\n",
       " '(reuters)',\n",
       " 'reuters',\n",
       " '-',\n",
       " 'short-sellers,',\n",
       " 'wall',\n",
       " \"street's\",\n",
       " 'dwindling\\\\band',\n",
       " 'of',\n",
       " 'ultra-cynics,',\n",
       " 'are',\n",
       " 'seeing',\n",
       " 'green',\n",
       " 'again.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(train_data[0]['text']) # list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc88d0f6",
   "metadata": {},
   "source": [
    "#### Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e56ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter() # map each word to its number of appearances\n",
    "\n",
    "for item in train_data:\n",
    "    counter.update(tokenize(item[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a67af01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wall : 1375\n",
      "st. : 1192\n",
      "bears : 344\n",
      "claw : 17\n",
      "back : 3868\n",
      "into : 6628\n",
      "the : 203234\n"
     ]
    }
   ],
   "source": [
    "# index, (value, key)\n",
    "for i, (word, num) in enumerate(counter.items()):\n",
    "    print(f\"{word} : {num}\")\n",
    "    \n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b477bd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158733"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c93ff2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG[\"max_vocab_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8866008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19998"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building the vocab based on MAX_VOCAB_SIZE\n",
    "most_common = counter.most_common(CONFIG[\"max_vocab_size\"] - 2) # 2 custom vocab - PAD and UNK\n",
    "\n",
    "len(most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0a532f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<unk>')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG[\"pad_token\"], CONFIG[\"unk_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca8159dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    CONFIG[\"pad_token\"]: 0, # to pad all the sentences to the same length\n",
    "    CONFIG[\"unk_token\"]: 1  # for the unknown tokens\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c11412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make up the vocabulary\n",
    "\n",
    "for i, (word, _) in enumerate(most_common, start=2):\n",
    "    vocab[word] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96babc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab) == CONFIG[\"max_vocab_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6933f",
   "metadata": {},
   "source": [
    "#### Numericalization and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f0baa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    tokens = tokenize(text)\n",
    "\n",
    "    ids = [vocab.get(tok, vocab[CONFIG[\"unk_token\"]]) for tok in tokens]\n",
    "\n",
    "    # Padding or truncation\n",
    "    if len(ids) < CONFIG[\"max_seq_len\"]:\n",
    "        ids += [vocab[CONFIG[\"pad_token\"]]] * (CONFIG[\"max_seq_len\"] - len(ids))\n",
    "    else:\n",
    "        # truncate it\n",
    "        ids = ids[:CONFIG[\"max_seq_len\"]]\n",
    "\n",
    "    return torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9010a89",
   "metadata": {},
   "source": [
    "#### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a431b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGNewsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    x: Tensor of shape (MAX_SEQ_LEN, )\n",
    "    y: int (0-3) -> 4 classes\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        x = encode(item[\"text\"])\n",
    "        y = item[\"label\"]\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f87457bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataloader\n",
    "\n",
    "\n",
    "train_dataset = AGNewsDataset(train_data)\n",
    "\n",
    "# batching the dataset\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48320037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([  369,   441,  1697, 17026,    98,    53,     2,   837,    29,    82,\n",
      "           10,     1,   369,  7034,     1,     5,     1,    35,  3922,   743,\n",
      "         2577,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 2)\n"
     ]
    }
   ],
   "source": [
    "for item in train_dataset:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de00ef5c",
   "metadata": {},
   "source": [
    "#### Positional Encoding\n",
    "\n",
    "* Transformers are parallel processing, so we use positional encoding\n",
    "* Why not integers, 0 to n:\n",
    "  * Scale: What if indices go very long, it will destabilize NN\n",
    "  * Generalization: Model trained on sequences of len 50 won't know what to do with sequences of length 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e605656a",
   "metadata": {},
   "source": [
    "- 128 - maximum length of the vector\n",
    "- 512 - embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bef23367",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding.\n",
    "    Injects word order information into embeddings coz transformer process everything in parallel.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len):\n",
    "        # d_model : total dim of the embedding\n",
    "        super().__init__()\n",
    "        \n",
    "        # position of each vector \n",
    "        pe = torch.zeros(max_len, d_model) # (max_seq_len, embedding_dim)\n",
    "        \n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        \n",
    "        # sine frequencies .. it generates unique value for all the tokens\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model) # it's a formula\n",
    "        )\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1d47ab",
   "metadata": {},
   "source": [
    "#### Self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa91304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    # For this word, which other words matter, and how much?\n",
    "    \n",
    "    \"\"\"\n",
    "    Every word in a sentence, \n",
    "        * Looks at every other word\n",
    "        * Scores their relevance\n",
    "        * Collects useful information\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        (Q) Query   :  What am I looking for?       - search for one thing\n",
    "        (K) Key     :  What do I contain?           - match based on another key\n",
    "        (V) Value   :  What information do I give?  - extract something else\n",
    "        \"\"\"\n",
    "        \n",
    "        # learnable linear layer\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.scale = math.sqrt(d_model)\n",
    "    \n",
    "    def forward(self, x): # x - input\n",
    "        \n",
    "        # for each input\n",
    "        Q = self.q(x)\n",
    "        K = self.k(x)\n",
    "        V = self.v(x)\n",
    "        \n",
    "        # Attention scores: Q * K^T / sqrt(d_model)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        return torch.matmul(weights, V)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eed975e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, d_model, ff_dim): # ff_dim -- hidden dimension of the MLP\n",
    "        super().__init__()\n",
    "        \n",
    "        # self attention layer\n",
    "        self.attention = SelfAttention(d_model) # give the weights each and every token\n",
    "        \n",
    "        # layer normalization after attention\n",
    "        self.norm1 = nn.LayerNorm(d_model) # stability during training\n",
    "        \n",
    "        # learning process\n",
    "        # position-wise feed-forward network\n",
    "        self.ff = nn.Sequential(            # MLP - Multi Layer Perceptron\n",
    "            nn.Linear(in_features=d_model, out_features=ff_dim),\n",
    "            nn.ReLU(),  # regularization .. max(0, n)\n",
    "            # linear computation\n",
    "            nn.Linear(in_features=ff_dim, out_features=d_model),\n",
    "        )\n",
    "        \n",
    "        # layer normalization after feed forward\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "            \n",
    "    def forward(self, x): # x - input\n",
    "        # First, we get the attention output\n",
    "        # first output\n",
    "        attn_out = self.attention(x) # weights of input tokens\n",
    "        \n",
    "        # we get the residual connection - to retain the earlier information\n",
    "        residual1 = x + attn_out\n",
    "        \n",
    "        # first normalization\n",
    "        x = self.norm1(residual1) # with residual connection\n",
    "        \n",
    "        # MLP -- learning mechanism\n",
    "        ff_out = self.ff(x) # x - input sentence\n",
    "        \n",
    "        residual2 = x + ff_out\n",
    "        x = self.norm2(residual2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c80cc49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Full transformer-based text classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, ff_dim, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # positional encoding\n",
    "        self.position_encoding = PositionalEncoding(d_model, CONFIG[\"max_seq_len\"])\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, ff_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x): # x - token IDs\n",
    "        # x.shape() = (batch_size, max_seq_len)\n",
    "        \n",
    "        # we embed the input\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # we generate positional encoding\n",
    "        x = self.position_encoding(x)\n",
    "        \n",
    "        # we feed to do MLP (Transformer Blocks (xN))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Global mean.. sequence pooling\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        # we do the classification\n",
    "        return self.classifier(x) # output - LOGITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d314882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerClassifier(\n",
    "    vocab_size=CONFIG[\"max_vocab_size\"],\n",
    "    d_model=128, # size of the embedding\n",
    "    ff_dim=256, # hidden size\n",
    "    num_layers=2,\n",
    "    num_classes=CONFIG[\"num_classes\"] # 4\n",
    ").to(CONFIG[\"device\"]) # cpu or gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55f743fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (embedding): Embedding(20000, 128)\n",
       "  (position_encoding): PositionalEncoding()\n",
       "  (layers): ModuleList(\n",
       "    (0-1): 2 x TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (v): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27e2e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader)) # 8 samples in one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd55d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = batch\n",
    "\n",
    "x = x.to(CONFIG[\"device\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27e7fef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "192679a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(x) # before sigmoid/softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b679f819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape # 4 classes, 8 samples per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "390d269c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5783, -0.3435,  0.1400, -0.0409],\n",
       "        [-0.5420, -0.3551,  0.1689, -0.0425],\n",
       "        [-0.5654, -0.3405,  0.1740,  0.0239],\n",
       "        [-0.6551, -0.3964,  0.1241,  0.0030],\n",
       "        [-0.5374, -0.3200,  0.1583, -0.0247],\n",
       "        [-0.4602, -0.3170,  0.2637, -0.0442],\n",
       "        [-0.5768, -0.2853,  0.1379, -0.0125],\n",
       "        [-0.6786, -0.3605,  0.1875,  0.0436]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5a7a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "\n",
    "test_dataset = AGNewsDataset(test_data)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3da8732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc2dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=3e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7288c92c",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48383993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # 1. Forward pass\n",
    "        logits = model(x)\n",
    "        \n",
    "        # 2. Compute loss\n",
    "        loss = criterion(logits, y) # model outputs, true labels\n",
    "        \n",
    "        \"\"\"BACKPROP.. this is the learning to correct the model\"\"\"\n",
    "        # 3. Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 4. Metrics\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct += (predictions == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d7639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct += (predictions == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54a764",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_one_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m EPOCHS = \u001b[32m5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     train_loss, train_acc = \u001b[43mtrain_one_epoch\u001b[49m(\n\u001b[32m      5\u001b[39m         model,\n\u001b[32m      6\u001b[39m         train_loader,\n\u001b[32m      7\u001b[39m         optimizer,\n\u001b[32m      8\u001b[39m         criterion,\n\u001b[32m      9\u001b[39m         CONFIG[\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m     )\n\u001b[32m     12\u001b[39m     val_loss, val_acc = evaluate(\n\u001b[32m     13\u001b[39m         model,\n\u001b[32m     14\u001b[39m         test_loader,\n\u001b[32m     15\u001b[39m         criterion,\n\u001b[32m     16\u001b[39m         CONFIG[\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     17\u001b[39m     )\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     20\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVal Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'train_one_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        CONFIG[\"device\"]\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc = evaluate(\n",
    "        model,\n",
    "        test_loader,\n",
    "        criterion,\n",
    "        CONFIG[\"device\"]\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8929f0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
