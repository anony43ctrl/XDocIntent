{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5f67bb",
   "metadata": {},
   "source": [
    "### Transformer Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e25622",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eaaf76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/love/miniconda3/envs/dl-research/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# System dependencies\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Torch dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Data loaders\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a91279",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65b8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    ### Data config\n",
    "    \"num_classes\": 4,\n",
    "    \n",
    "    ### NLP config\n",
    "    \"max_vocab_size\": 20000,\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"max_seq_len\": 128,\n",
    "    \n",
    "    ### system settings\n",
    "    \"seed\": 42,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b41061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    torch.manual_seed(CONFIG[\"seed\"])\n",
    "    \n",
    "setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c516e31",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce625437",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "label_map = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c971714",
   "metadata": {},
   "source": [
    "#### Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b004f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer\n",
    "\n",
    "def tokenize(text):\n",
    "    # text.lower() - lowercase all the text samples\n",
    "    # text.split() - split the sentence into words\n",
    "    return text.lower().split() # [\"word1\", \"word2\", \"word3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a03b1271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]['text'] # sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ca04239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wall',\n",
       " 'st.',\n",
       " 'bears',\n",
       " 'claw',\n",
       " 'back',\n",
       " 'into',\n",
       " 'the',\n",
       " 'black',\n",
       " '(reuters)',\n",
       " 'reuters',\n",
       " '-',\n",
       " 'short-sellers,',\n",
       " 'wall',\n",
       " \"street's\",\n",
       " 'dwindling\\\\band',\n",
       " 'of',\n",
       " 'ultra-cynics,',\n",
       " 'are',\n",
       " 'seeing',\n",
       " 'green',\n",
       " 'again.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(train_data[0]['text']) # list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc88d0f6",
   "metadata": {},
   "source": [
    "#### Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e56ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter() # map each word to its number of appearances\n",
    "\n",
    "for item in train_data:\n",
    "    counter.update(tokenize(item[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a67af01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wall : 1375\n",
      "st. : 1192\n",
      "bears : 344\n",
      "claw : 17\n",
      "back : 3868\n",
      "into : 6628\n",
      "the : 203234\n"
     ]
    }
   ],
   "source": [
    "# index, (value, key)\n",
    "for i, (word, num) in enumerate(counter.items()):\n",
    "    print(f\"{word} : {num}\")\n",
    "    \n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b477bd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158733"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c93ff2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG[\"max_vocab_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8866008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19998"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building the vocab based on MAX_VOCAB_SIZE\n",
    "most_common = counter.most_common(CONFIG[\"max_vocab_size\"] - 2) # 2 custom vocab - PAD and UNK\n",
    "\n",
    "len(most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0a532f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<unk>')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG[\"pad_token\"], CONFIG[\"unk_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca8159dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    CONFIG[\"pad_token\"]: 0, # to pad all the sentences to the same length\n",
    "    CONFIG[\"unk_token\"]: 1  # for the unknown tokens\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c11412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make up the vocabulary\n",
    "\n",
    "for i, (word, _) in enumerate(most_common, start=2):\n",
    "    vocab[word] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96babc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab) == CONFIG[\"max_vocab_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6933f",
   "metadata": {},
   "source": [
    "#### Numericalization and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f0baa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    tokens = tokenize(text)\n",
    "\n",
    "    ids = [vocab.get(tok, vocab[CONFIG[\"unk_token\"]]) for tok in tokens]\n",
    "\n",
    "    # Padding or truncation\n",
    "    if len(ids) < CONFIG[\"max_seq_len\"]:\n",
    "        ids += [vocab[CONFIG[\"pad_token\"]]] * (CONFIG[\"max_seq_len\"] - len(ids))\n",
    "    else:\n",
    "        # truncate it\n",
    "        ids = ids[:CONFIG[\"max_seq_len\"]]\n",
    "\n",
    "    return torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9010a89",
   "metadata": {},
   "source": [
    "#### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a431b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGNewsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    x: Tensor of shape (MAX_SEQ_LEN, )\n",
    "    y: int (0-3) -> 4 classes\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        x = encode(item[\"text\"])\n",
    "        y = item[\"label\"]\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f87457bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataloader\n",
    "\n",
    "\n",
    "train_dataset = AGNewsDataset(train_data)\n",
    "\n",
    "# batching the dataset\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de00ef5c",
   "metadata": {},
   "source": [
    "#### Positional Encoding\n",
    "\n",
    "* Transformers are parallel processing, so we use positional encoding\n",
    "* Why not integers, 0 to n:\n",
    "  * Scale: What if indices go very long, it will destabilize NN\n",
    "  * Generalization: Model trained on sequences of len 50 won't know what to do with sequences of length 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e605656a",
   "metadata": {},
   "source": [
    "- 128 - maximum length of the vector\n",
    "- 512 - embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bef23367",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding.\n",
    "    Injects word order information into embeddings coz transformer process everything in parallel.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len):\n",
    "        # d_model : total dim of the embedding\n",
    "        super().__init__()\n",
    "        \n",
    "        # position of each vector \n",
    "        pe = torch.zeros(max_len, d_model) # (max_seq_len, embedding_dim)\n",
    "        \n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        \n",
    "        # sine frequencies .. it generates unique value for all the tokens\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model) # it's a formula\n",
    "        )\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1d47ab",
   "metadata": {},
   "source": [
    "#### Self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa91304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    # For this word, which other words matter, and how much?\n",
    "    \n",
    "    \"\"\"\n",
    "    Every word in a sentence, \n",
    "        * Looks at every other word\n",
    "        * Scores their relevance\n",
    "        * Collects useful information\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        (Q) Query   :  What am I looking for?       - search for one thing\n",
    "        (K) Key     :  What do I contain?           - match based on another key\n",
    "        (V) Value   :  What information do I give?  - extract something else\n",
    "        \"\"\"\n",
    "        \n",
    "        # learnable linear layer\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.scale = math.sqrt(d_model)\n",
    "    \n",
    "    def forward(self, x): # x - input\n",
    "        \n",
    "        # for each input\n",
    "        Q = self.q(x)\n",
    "        K = self.k(x)\n",
    "        V = self.v(x)\n",
    "        \n",
    "        # Attention scores: Q * K^T / sqrt(d_model)\n",
    "        scores = torch.matmul(Q, K.transpose(-2 -1)) / self.scale\n",
    "        \n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        return torch.matmul(weights, V)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed975e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, d_model, ff_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self attention layer\n",
    "        self.attention = SelfAttention(d_model) # give the weights each and every token\n",
    "        \n",
    "        # layer normalization after attention\n",
    "        self.norm1 = nn.LayerNorm(d_model) # stability during training\n",
    "        \n",
    "        # learning process\n",
    "        # position-wise feed-forward network\n",
    "        self.ff = nn.Sequential(            # MLP - Multi Layer Perceptron\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.ReLU(),  # regularization .. max(0, n)\n",
    "            nn.Linear(ff_dim, d_model)\n",
    "        )\n",
    "        \n",
    "        # layer normalization after feed forward\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "            \n",
    "    def forward(self, x): # x - input\n",
    "        attn_out = self.attention(x)\n",
    "        x = self.norm1(x + attn_out) # with residual connection\n",
    "        \n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c80cc49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Full transformer-based text classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, ff_dim, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8aa6da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
